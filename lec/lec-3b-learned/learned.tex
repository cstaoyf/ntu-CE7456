\include{./def/yf-formatting}
\include{./def/yf-def}

\title{}
\author{}
\date{}



\def\bD{\mathbb{D}}
\def\bQ{\mathbb{Q}}

\def\A{\mathcal{A}}
\def\B{\mathcal{B}}
\def\C{\mathcal{C}}
\def\D{\mathcal{D}}
\def\E{\mathcal{E}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\I{\mathcal{I}}
\def\L{\mathcal{L}}
\def\R{\mathcal{R}}
\def\T{\mathcal{T}}
%\def\U{\mathcal{U}}
%\def\X{\mathcal{X}}
\def\rank{\mit{rank}}
\def\tm{\mit{Time}}

\def\extraspacing{\vspace{3mm} \noindent}
\def\vgap{\vspace{3mm}}
\def\vslit{\vspace{0.5mm}}

\DeclareMathOperator*{\Log}{Log}

\begin{document}

\boxminipg{\linewidth}{
    \begin{center}
        \vspace{3mm}
    {\Large
    (CE7456 Lecture Notes 8) Distribution-Aware Data Structuring} \\[1mm]
    {\Large (a.k.a.\ Learned Indexes)} \\[2mm]
    {\large Yufei Tao}
    \vspace{3mm}
    \end{center}
}

So far all the complexities in our algorithmic analysis are claimed in the {\em worst case}. For example, by saying that the binary search tree (BST) consumes $O(n)$ space and answers a predecessor search in $O(\log n)$ time, we require these complexities to hold on {\em every} set of $n$ integers and {\em every} query. Today, we will switch to a different form of analysis for deriving {\em average complexities}, which hold in expectation when data and query follow specific distributions.

\vgap

We still use predecessor search as the representative example: the input (i.e., the dataset) is a collection $S$ of $n$ integers; given an integer $q$, a query returns the the largest element $e \in S$ satisfying $e \le q$, or declares that such an element does not exist. All the integers --- including those in $S$ and the query value $q$ --- are assumed to fall in the domain $[U]$ (for an integer $x > 0$, the notation $[x]$ represents the set $\set{1, 2, ..., x}$) for some (large) integer $U$. W.l.o.g., we consider that $U$ is a multiple of $n$ (if not, increase $U$ to make the property hold).

\vgap

Let $\D$ be a probabilistic distribution over $[U]$. We consider that
\myitems{
    \item the $n$ integers of $S$ are drawn independently from $\D$ (namely, $S$ is drawn from $\D^n$);
    \item the query value $q$ is drawn from $\D$ and is independent from $S$.
}
Let $\tm_S(Q)$ be the time of an algorithm $\A$ for answering a query with search value $Q$ on a dataset $S$. Note that both $S$ and $Q$ are random variables and, hence, so is $\tm_S(Q)$. We define the {\em average time complexity} of $\A$ over $\D$ as
\myeqn{
    \expt_{Q, S} [\tm_S(Q)]. \nn
}
The worst-case $O(\log n)$ guarantee of BST directly implies an average query time of $O(\log n)$. The challenge is to find a data structure with a lower average query time on ``easy'' distributions $\D$.

\section{Structure} \label{sec:str}

Define
\myeqn{
    a &=& \min_{e \in S} e \nn \\
    b &=& \max_{e \in S} e. \nn
}
We consider $a < b$ (the problem is trivial otherwise).
Use an array $A$ to store the elements of $S$ in ascending order. Divide the range $[a, b]$ into $n$ intervals with equal length; specifically, the $i$-th interval ($i \in [n]$) is
\myeqn{
    I_i &=& \left\{
    \begin{tabular}{ll}
        $\Big[a + (b-a) \cdot \fr{i-1}{n}, a + (b-a) \cdot \fr{i}{n} \Big)$ & if $i < n$  \\[2mm]
        $\Big[a + (b-a) \cdot \fr{i-1}{n}, a + (b-a) \cdot \fr{i}{n} \Big]$ & if $i = n$.
    \end{tabular}
    \right.
    \label{eqn:Ii}
}
For each $i \in [n]$, define
\myeqn{
    n_i &=& |S \cap I_i|. \label{eqn:ni}
}
and a function $\hat{r}: [U] \rightarrow [n]$ as:
\myeqn{
    \hat{r}(q)
    &=&
    \left\{
    \begin{tabular} {ll}
        0 & if $q < a$ \\
        $\fr{n_i}{2} + \sum_{k=1}^{i-1} n_i$ & if $q \in I_i$ for some $i \in [n]$ \\
        $n$ &
        if $q > b$
    \end{tabular}
    \right. \label{eqn:approx-r}
}

Our data structure consists of (i) the values $a, b$ and $n$, (ii) the sorted array $A$, and the function $\hat{r}$. In particular, we store $\hat{r}$ in an array of size $n$ such that, for any $q \in I_i$ with any $i \in [n]$, we can return $\hat{r}(q)$ in constant time. The total space consumption is $O(n)$ in the worst case.

\section{Query} \label{sec:query}

\section{Analysis} \label{sec:analysis}

Define
\myeqn{
    \Delta_S(q) &=& |\rank_S(q) - \hat{r}_S(q)|. \nn
}
Partition the data domain $[0, U]$ into $n$ intervals of equal length; specifically, for each $j \in [n]$, the $j$-th interval is
\myeqn{
    J_j &=& [(j-1)/n, j/n]. \label{eqn:analysis-J-j}
}
Specially, define
\myeqn{
    J_0 = J_{n+1} = \emptyset. \nn
}
Set
\myeqn{
    m_j &=& |S \cap J_j|. \label{eqn:analysis-m-j}
}
for each $j \in \set{0, 1, ..., n+1}$.
\todo{comparison remark}

\vgap

The core of our analysis is to prove:

\begin{lemma} \label{lmm:anayslis-core}
    $\expt_{Q, S} [\Delta_S(Q)] = O(n) \cdot \sum_{j=1}^n p_j^2$ where $p_j = \Pr_{X \sim \D} [X \in J_j]$ for each $j \in [n]$.
\end{lemma}

\extraspacing {\bf Proof of Lemma~\ref{lmm:anayslis-core}.} By the total law of expectation, we have:
\myeqn{
    \expt_{Q, S}[\Delta_S(Q)] &=& \expt_S \Big[\expt_{Q \mid S} \Big[\Delta_S(Q) | S \Big] \Big] \nn  \\
    \explain{by independence of $Q$ and $S$} &=& \expt_S \Big[\expt_{Q} \Big[\Delta_S(Q) | S \Big] \Big].
    \label{eqn:analysis-core-h1}
}
The subsequent discussion will focus on $\expt_Q [\Delta_S(Q) | S]$, for which we have:
\myeqn{
    \expt_Q [\Delta_S(Q) | S]
    &=&
    \int_0^U \Delta_S(q) \cdot f(q) dq \nn \\
    &=&
    \sum_{i=1}^n \int_{I_i} \Delta_S(q) \cdot f(q) dq \nn \\
    \explain{\todo{reason}}
    &\le&
    \sum_{i=1}^n \fr{n_i}{2} \int_{I_i} f(q) dq \label{eqn:analysis-core-h2}
}
where --- let us recall --- $n_i = |S \cap I_i|$. Define for each $i, j \in [n]$:
\myeqn{
    n_{i,j} = |S \cap I_i \cap J_j|. \label{eqn:analysis-n-ij} \nn
}
Thus
\myeqn{
    \eqref{eqn:analysis-core-h2} =
    \fr{1}{2} \sum_{i=1}^n \sum_{j=1}^n n_{i,j} \int_{I_i} f(q) dq
    =
    \fr{1}{2} \sum_{j=1}^n \sum_{i=1}^n n_{i,j} \int_{I_i} f(q) dq.
    \label{eqn:analysis-core-h3}
}
For a specific $j \in [n]$, we have
\myeqn{
    \sum_{i=1}^n n_{i,j} \int_{I_i} f(q) dq
    &=&
    \sum_{i \in [n]: I_i \cap J_j \ne \emptyset} n_{i,j} \int_{I_i} f(q) dq \nn \\
    \explain{\todo{}}
    &\le&
    \sum_{i \in [n]: I_i \cap J_j \ne \emptyset} n_{i,j} \int_{J_{j-1} \cup J_j \cup J_{j+1}} f(q) dq \nn \\
    &=&
    m_j \cdot (p_{j-1} + p_j + p_{j+1}). \nn
}
Plugging the above into \eqref{eqn:analysis-core-h3} and then into \eqref{eqn:analysis-core-h2} yields
\myeqn{
    \expt_Q [\Delta_S(Q) | S] =
    \fr{1}{2} \sum_{j=1}^n  m_j \cdot (p_{j-1} + p_j + p_{j+1}) \nn
}
which, together with \eqref{eqn:analysis-core-h1}, gives
\myeqn{
    \expt_{Q,S} [\Delta_S(Q) | S] = \expt_S [\expt_Q [\Delta_S(Q) | S]] &=&
    \fr{1}{2} \cdot \expt_S \Big[ \sum_{j=1}^n  m_j \cdot (p_{j-1} + p_j + p_{j+1}) \Big] \nn \\
    &=&
    \fr{1}{2} \cdot \sum_{j=1}^n  \expt_S[m_j] \cdot (p_{j-1} + p_j + p_{j+1}) \nn \\
    &=&
    \fr{1}{2} \cdot \sum_{j=1}^n  n p_j \cdot (p_{j-1} + p_j + p_{j+1}) \nn \\
    &=&
    \fr{n}{2} \cdot \sum_{j=1}^n  p_j p_{j-1} + p_j^2 + p_j p_{j+1}. \label{eqn:analysis-core-h4}
}

The proof of the next fact is left to you as an exercise:

\begin{proposition}
    $\sum_{j=1}^n  p_j p_{j-1} = O(\sum_{j=1}^n p_j^2)$ and $\sum_{j=1}^n  p_j p_{j+1} = O(\sum_{j=1}^n p_j^2)$.
\end{proposition}

Lemma~\ref{lmm:anayslis-core} now follows by combining the above proposition and \eqref{eqn:analysis-core-h4}.


\appendix

\section{Exponential Search}

\section{Exponential Search}

\subsection{Problem Setting}

Let $A$ be a sorted array of $n$ elements (in non-decreasing order),
and let $q$ be a target value. We wish to determine whether $q$ appears in $A$,
and if so, return its index.

\extraspacing{\bf Algorithm.} If $q < A[1]$, return ``absence''. Otherwise, identify the smallest $h \in [n]$ such that
\myitems{
    \item $h$ is a power of 2 or $h = n$, and
    \item $A[n] \ge q$
}
By definition of $h$, the value $q$ --- if it exists in $A$ --- must lie in the subarray from $A[\lc h/2 \rc]$ to $A[h]$. We then perform binary search to look for $q$ in the subarray.

\extraspacing {\bf Analysis.} Let $\Delta$ be the number of elements in $A$ that are at most $q$. Clearly, $h \le 2\Delta$. It is rudimentary to show that the running time is $O(\log \Delta)$.


\bibliographystyle{plain}
\bibliography{ref}

\end{document}
