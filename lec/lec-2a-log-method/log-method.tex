\include{./def/yf-formatting}
\include{./def/yf-def}

\title{}
\author{}
\date{}


\def\InvS{\textrm{InvS}}
\def\deg{\textrm{deg}}

\def\bD{\mathbb{D}}
\def\bQ{\mathbb{Q}}

% \def\A{\mathcal{A}}
\def\B{\mathcal{B}}
\def\C{\mathcal{C}}
\def\Cb{\mathbb{C}}
\def\E{\mathcal{E}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\I{\mathcal{I}}
\def\L{\mathcal{L}}
\def\R{\mathcal{R}}
\def\T{\mathcal{T}}
%\def\U{\mathcal{U}}
%\def\X{\mathcal{X}}

\def\ans{\textsc{Ans}}

\def\extraspacing{\vspace{3mm} \noindent}
\def\vgap{\vspace{3mm}}
\def\vslit{\vspace{0.5mm}}

\begin{document}

% \begin{center}
%     {\bf \large Lecture Notes} \\
%     {\bf \large Approximate Nearest Neighbor Search 1 : Proximity Graphs} \\[5mm]
%     {\large Yufei Tao} \\[10mm]
% \end{center}

\boxminipg{\linewidth}{
    \begin{center}
        \vspace{3mm}
    {\Large
    (CE7456 Lecture Notes 5) The Logarithmic Method} \\[2mm]
    {\large Yufei Tao}
        \vspace{3mm}
    \end{center}
}


%\chapter{The Logarithmic Method} \label{lec:rebuild}

A data structure is said to be
\myitems{
    \item {\em static} if it does not support any updates;
    \item {\em semi-dynamic} if it supports only insertions;
    \item {\em fully-dynamic} if it supports both insertions and deletions.
}
Today, we will learn a generic technique --- called the {\em logarithmic method} --- that can turn a static structure into a semi-dynamic one.

%We will use the kd-tree (Section~\ref{sec:pt:kd}) to illustrate the technique. Indeed, the kd-tree serves as an excellent example because it may seem exceedingly difficult to modify the structure for updates. For example, the first cut in a kd-tree --- let us recall --- ought to be a vertical line that divides the point set as evenly as possible. Unfortunately, even a single point insertion would throw off the balance and thus destroy the whole tree. It may be surprising to you that later we will make the kd-tree semi-dynamic without changing the structure at all.


\section{Decomposable Problems} \label{sec:rebuild:decomp}

%This section will formalize the class of problems to be investigated.

Let $\bD$ be a (possibly infinite) set called the {\em data domain}, whose members are called {\em data elements}. Let $\bQ$ be a (possibly infinite) set called the {\em query domain}, whose members are called {\em queries}. Over a finite $S \subseteq \bD$, each query $q \in \bQ$ returns an {\em answer}, represented as $\ans_q(S)$. In a {\em problem} characterized by $(\bD, \bQ)$, the goal is to store a given set $S \subseteq \bD$ --- where $S$ is called the {\em dataset} --- in a data structure that allows us to compute $\ans_q(S)$ efficiently for any $q \in \bQ$.

\vgap

A problem characterized by $(\bD, \bQ)$ is {\em decomposable} if, for any disjoint sets $S_1, S_2 \subseteq \mathbb{D}$ and any query $q \in \mathbb{Q}$, it is possible to derive $\ans_q(S_1 \cup S_2)$ from $\ans_q(S_1)$ and $ \ans_q(S_2)$ in constant time.

\vgap

Consider, for example, 1D range reporting, where $S$ is a set of integers, a query $q$ is an interval, and $\ans_q(S)$ is $S \cap q$, i.e., the set of integers in $S$ covered by $q$. As another example, in 1D range counting, the meanings of $S$ and $q$ are identical to those  in 1D range reporting, but $\ans_q(S)$ is $|S \cap q|$, i.e., how many integers of $S$ are covered by $q$. Both problems are decomposable.

\section{The Logarithmic Method} \label{sec:rebuild:log}

We will prove:

\begin{theorem} \label{thm:log:main}
    Suppose that there is a static structure $\Upsilon$ that
    \myitems{
        \item stores $n$ elements in $F(n)$ space;
        \item can be constructed in $n \cdot U(n)$ time;
        \item answers a decomposable query in $Q(n)$ time (plus, if necessary, a cost linear to the number of reported elements).
    }
    Set $h = \lc \log_2 n \rc$. There is a semi-dynamic structure $\Upsilon'$ that
    \myitems{
        \item stores $n$ elements in $\sum_{i=0}^h F(2^i)$ space;
        \item supports an insertion in $O\left( \sum_{i=0}^h U(2^i) \right)$ amortized time;
        \item answers a decomposable query in $O(\log n) + \sum_{i=0}^h Q(2^i)$ time (plus, if necessary, a cost linear to the number of reported elements)
    }


    %If in addition $\Upsilon$ also supports a deletion in at most $U_\mit{del}(n)$ worst-case time, then $\Upsilon'$ is a fully dynamic structure that supports a deletion in $O(\log n) + U_\mit{del}(n)$ worst-case time, and retains all the above properties.
\end{theorem}

\vgap

Before delving into the proof, let us first see an application. Suppose that we have a structure that can be constructed in $O(n \log n)$ time and answers a query in $O(\sqrt{n})$ time. Therefore:
\myeqn{
    F(n) &=& O(n) \nn \\
    U(n) &=& O(\log n) \nn \\
    Q(n) &=& O(\sqrt{n}). \nn
}
Theorem~\ref{thm:log:main} immediately gives a semi-dynamic structure that uses
\myeqn{
    \sum_{i=0}^{\lc \log_2 n \rc} O(2^i) &=& O(n) \nn
}
space, supports an insertion in
\myeqn{
    \sum_{i=0}^{\lc \log_2 n \rc} O\left(\log 2^i\right) &=& O(\log^2 n) \nn
}
amortized time, and answers a query in
\myeqn{
    \sum_{i=0}^{\lc \log_2 n \rc} O\left(\sqrt{2^i}\right) &=& O(\sqrt{n}) \nn
}
time.

%Even better, later we will show how to support a deletion in the kd-tree using $O(\log n)$ worst-case time. The theorem thus yields a fully dynamic structure with all the above performance guarantees and at the same time the ability to support a deletion in $O(\log n)$ worst-case time.

\subsection{Structure} \label{sec:rebuild:log:str}

Let $S$ be the input set of elements; let $n = |S|$ and $h = \lc \log_2 n \rc$. At all times, we divide $S$ into disjoint subsets $S_0, S_1, ..., S_h$ (some of which may be empty) satisfying:
\myeqn{
    |S_i| &\le& 2^i. \label{eqn:rebuild:log:size}
}
Create a structure of $\Upsilon$ on each subset; denote by $\Upsilon_i$ the structure on $S_i$. Then, $\Upsilon_1, \Upsilon_2, ..., \Upsilon_h$ together constitute our semi-dynamic structure. The space usage is bounded by $\sum_{i=0}^h F(2^i)$.

\vgap

Before receiving any updates, $S_0 = S_1 = ... = S_{h-1} = \emptyset$ and $S_h = S$. Accordingly, $\Upsilon_0, ..., \Upsilon_{h-1}$ are empty and $\Upsilon_h$ stores the entire $S$.


\subsection{Query} \label{sec:rebuild:log:qry}

To answer a query $q$, we simply search all of $\Upsilon_1, ..., \Upsilon_h$. Since the query is decomposable, we can obtain the answer on $S$ from the answers on $S_1, ..., S_h$ in $O(h)$ time. The overall query time is
\myeqn{
    O(h) + \sum_{i=0}^h Q(2^i) = O(\log n) + \sum_{i=0}^h Q(2^i). \nn
}

\subsection{Insertion} \label{sec:rebuild:log:ins}

To insert an element $e_\mit{new}$, we first identify the smallest $i \in [0, h]$ satisfying:
\myeqn{
    1 + \sum_{j=0}^i |S_j| &\le& 2^i \label{eqn:rebuild:log:i}.
}
We now proceed as follows:
\myitems{
    \item If $i$ exists, we destroy $\Upsilon_0, \Upsilon_1, ..., \Upsilon_i$ and move all the elements in $S_0, S_1, ..., S_{i-1}$, together with $e_\mit{new}$, {\em into} $S_i$. After this, $S_0, S_1, ..., S_{i-1}$ are empty and $S_i$ contains all their elements, the elements that were already in $S_i$ before, and $e_\mit{new}$. Rebuild $\Upsilon_i$ on the $S_i$ from scratch.

    \item If $i$ does not exist, we destroy $\Upsilon_0, \Upsilon_1, ..., \Upsilon_h$, and move all the elements in $S_0, S_1, ..., S_h$, together with $e_\mit{new}$, {\em into} $S_{h+1}$. Build $\Upsilon_{h+1}$ on $S_{h+1}$ from scratch. The value of $h$ then increases by 1.
}

Let us now analyze the amortized insertion cost with a charging argument. Each time $\Upsilon_i$ ($i \ge 0$) is rebuilt, we spend
\myeqn{
    O(|S_i|) \cdot U(|S_i|) &=& O(2^i) \cdot U(2^i) \label{eqn:rebuild:log:rebuildcost}
}
cost (recall that, as stated in Theorem~\ref{thm:log:main}, a structure on $n$ elements can be built in $n \cdot U(n)$ time). The lemma below gives a crucial observation:

\begin{lemma} \label{lmm:rebuild:log:rebuild}
    Every time when $\Upsilon_i$ is rebuilt, at least $1 + 2^{i-1}$ elements are added to $S_i$ (i.e., every such element was in some $S_j$ with $j < i$).
\end{lemma}

\begin{proof}
    Set $\lambda = i$. By our choice of $i$, the inequality \eqref{eqn:rebuild:log:i} does not hold for $i = \lambda - 1$. This means:
    \myeqn{
        1 + \sum_{j=0}^{\lambda-1} |S_j| &\ge& 1 + 2^{\lambda-1}. \nn
    }
    This proves the claim because all the elements in $S_1, ..., S_{\lambda-1}$, as well as $e_\mit{new}$, are added to $S_\lambda$.
\end{proof}

We can therefore charge the cost of rebuilding $\Upsilon_i$  --- namely the cost in \eqref{eqn:rebuild:log:rebuildcost} --- on the at least $2^{i-1}$ elements added to $S_i$, such that each of those elements bears only
\myeqn{
    \fr{O(2^i)}{2^{i-1}} \cdot U(2^i) &=& O(U(2^i)) \nn
}
cost.

\vgap

In other words, every time an element $e$ moves to new $S_i$, it bears a cost of $O(U(2^i))$. Note that an element never moves from $S_i$ to an $S_j$ with $j < i$. Therefore, $e$ can be charged at most $h + 1$ times with a total cost of
\myeqn{
    O\left(
        \sum_{i=0}^h U(2^i). \nn
    \right)
}
We have proved that any sequence of $m$ insertions can be processed in
\myeqn{
    O\left(
        m \cdot \sum_{i=0}^h U(2^i) \nn
    \right)
}
time.

\section{Remarks}

The logarithmic method was developed by Bentley and Saxe \cite{bs80}. There are standard {\em de-amortization} techniques (see \cite{o87}) that convert a structure with small amortized update time into a structure achieving a small time bound on {\em every} update. By applying those techniques, we can turn our modified kd-tree into a structure that ensures $O(\log^2 n)$ time on every insertion.


\bibliographystyle{plain}
\bibliography{ref}

\end{document}
